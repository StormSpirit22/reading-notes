# 16 内存分配

## 16.1 概述

基本策略：

1. 每次从操作系统申请一大块内存（比如 1MB），以减少系统调用。
2. 将申请到的大块内存按照特定大小预先切分成小块，构成链表。
3. 为对象分配内存时，只需从大小合适的链表提取一个小块即可。
4. 回收对象内存时，将该小块内存重新归还到原链表，以便复用。
5. 如闲置内存过多，则尝试归还部分内存给操作系统，降低整体开销。

以下概述内容来自：[内存分配器](https://go.cyub.vip/memory/allocator.html#id1)

Golang内存分配管理策略是**按照不同大小的对象和不同的内存层级来分配管理内存**。通过这种多层级分配策略，形成无锁化或者降低锁的粒度，以及尽量减少内存碎片，来提高内存分配效率。

Golang中内存分配管理的对象按照大小可以分为：

| 类别                | 大小        |
| :------------------ | :---------- |
| 微对象 tiny object  | (0, 16B)    |
| 小对象 small object | [16B, 32KB] |
| 大对象 large object | (32KB, +∞)  |

Golang中内存管理的层级从最下到最上可以分为：mspan -> mcache -> mcentral -> mheap -> heapArena（等同于下面的 span、cache 等)。golang中对象的内存分配流程如下：

1. 小于16个字节的对象使用`mcache`的微对象分配器进行分配内存
2. 大小在16个字节到32k字节之间的对象，首先计算出需要使用的`span`大小规格，然后使用`mcache`中相同大小规格的`mspan`分配
3. 如果对应的大小规格在`mcache`中没有可用的`mspan`，则向`mcentral`申请
4. 如果`mcentral`中没有可用的`mspan`，则向`mheap`申请，并根据BestFit算法找到最合适的`mspan`。如果申请到的`mspan`超出申请大小，将会根据需求进行切分，以返回用户所需的页数，剩余的页构成一个新的`mspan`放回`mheap`的空闲列表
5. 如果`mheap`中没有可用`span`，则向操作系统申请一系列新的页（最小 1MB）
6. 对于大于32K的大对象直接从`mheap`分配

### 内存块

分配器将其管理的内存块分为两种。

- span：由多个地址连续的页组成的大块内存。
- object：将 span 按特定大小切分成多个小块，每个小块可存储一个对象。

> 按照其用途，span 面向内部管理， object 面向对象分配。

小对象分配会在 span page 中划分更小的粒度；大对象通过多页实现。

### 管理组件

Go 采用了 tcmalloc 的成熟架构。

分配器由三种组件组成：

- cache：每个运行期工作线程都会绑定一个 cache，用于无锁 object 分配。
- central：cache 中没有可用的 span 时候，会向 central 申请。
- heap：管理闲置 span，需要时向操作系统申请新内存。



#### 代码

Go 版本：go version go1.17 darwin/amd64

mheap.go:

```go
//go:notinheap
type mspan struct {
	next *mspan     // next span in list, or nil if none
	prev *mspan     // previous span in list, or nil if none
	list *mSpanList // For debugging. TODO: Remove.

	startAddr uintptr // address of first byte of span aka s.base()
	npages    uintptr // number of pages in span

	manualFreeList gclinkptr // list of free objects in mSpanManual spans
  ...
}

//go:notinheap
type mcache struct {
	// The following members are accessed on every malloc,
	// so they are grouped here for better caching.
	nextSample uintptr // trigger heap sample after allocating this many bytes
	scanAlloc  uintptr // bytes of scannable heap allocated

	// Allocator cache for tiny objects w/o pointers.
	// See "Tiny allocator" comment in malloc.go.

	// tiny points to the beginning of the current tiny block, or
	// nil if there is no current tiny block.
	//
	// tiny is a heap pointer. Since mcache is in non-GC'd memory,
	// we handle it by clearing it in releaseAll during mark
	// termination.
	//
	// tinyAllocs is the number of tiny allocations performed
	// by the P that owns this mcache.
	tiny       uintptr
	tinyoffset uintptr
	tinyAllocs uintptr

	// The rest is not accessed on every malloc.

	alloc [numSpanClasses]*mspan // spans to allocate from, indexed by spanClass

	stackcache [_NumStackOrders]stackfreelist

	// flushGen indicates the sweepgen during which this mcache
	// was last flushed. If flushGen != mheap_.sweepgen, the spans
	// in this mcache are stale and need to the flushed so they
	// can be swept. This is done in acquirep.
	flushGen uint32
}

// Central list of free objects of a given size.
//
//go:notinheap
type mcentral struct {
	spanclass spanClass

	// partial and full contain two mspan sets: one of swept in-use
	// spans, and one of unswept in-use spans. These two trade
	// roles on each GC cycle. The unswept set is drained either by
	// allocation or by the background sweeper in every GC cycle,
	// so only two roles are necessary.
	//
	// sweepgen is increased by 2 on each GC cycle, so the swept
	// spans are in partial[sweepgen/2%2] and the unswept spans are in
	// partial[1-sweepgen/2%2]. Sweeping pops spans from the
	// unswept set and pushes spans that are still in-use on the
	// swept set. Likewise, allocating an in-use span pushes it
	// on the swept set.
	//
	// Some parts of the sweeper can sweep arbitrary spans, and hence
	// can't remove them from the unswept set, but will add the span
	// to the appropriate swept list. As a result, the parts of the
	// sweeper and mcentral that do consume from the unswept list may
	// encounter swept spans, and these should be ignored.
	partial [2]spanSet // list of spans with a free object
	full    [2]spanSet // list of spans with no free objects
}


//go:notinheap
type mheap struct {
	// lock must only be acquired on the system stack, otherwise a g
	// could self-deadlock if its stack grows with the lock held.
	lock  mutex
	pages pageAlloc // page allocation data structure

	sweepgen     uint32 // sweep generation, see comment in mspan; written during STW
	sweepDrained uint32 // all spans are swept or are being swept
	sweepers     uint32 // number of active sweepone calls
  ...
}

//go:notinheap
type heapArena struct {
	// bitmap stores the pointer/scalar bitmap for the words in
	// this arena. See mbitmap.go for a description. Use the
	// heapBits type to access this.
	bitmap [heapArenaBitmapBytes]byte

	// spans maps from virtual address page ID within this arena to *mspan.
	// For allocated spans, their pages map to the span itself.
	// For free spans, only the lowest and highest pages map to the span itself.
	// Internal pages map to an arbitrary span.
	// For pages that have never been allocated, spans entries are nil.
	//
	// Modifications are protected by mheap.lock. Reads can be
	// performed without locking, but ONLY from indexes that are
	// known to contain in-use or stack spans. This means there
	// must not be a safe-point between establishing that an
	// address is live and looking it up in the spans array.
	spans [pagesPerArena]*mspan

	// pageInUse is a bitmap that indicates which spans are in
	// state mSpanInUse. This bitmap is indexed by page number,
	// but only the bit corresponding to the first page in each
	// span is used.
	//
	// Reads and writes are atomic.
	pageInUse [pagesPerArena / 8]uint8
	...
}


```



由于目前版本的 Go 内存部分源码与书中相差比较大，所以这一章会引用目前网上比较新的文章。
